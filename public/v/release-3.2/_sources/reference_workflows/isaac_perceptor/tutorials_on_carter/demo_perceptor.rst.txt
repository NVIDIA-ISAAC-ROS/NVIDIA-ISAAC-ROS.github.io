Tutorial: Camera-based 3D Perception with Isaac Perceptor on the `Nova Carter`
==================================================================================

.. figure:: :ir_lfs:`<resources/isaac_ngc/nova_carter/Nova_Carter_Isaac_KV_540p_01_v002_DM.png>`

This demo enables you to use the `Isaac Perceptor` to perceive the
robot's environment - purely based on cameras. `Isaac Perceptor` uses Isaac ROS components,
including Visual SLAM to localize the robot, ESS and Nvblox to reconstruct 3D environments.

For this tutorial, it is assumed that you have successfully completed the
:doc:`teleop tutorial</robots/nova_carter/demo_teleop>` and the
:doc:`extrinsic calibration instructions </getting_started/hardware_setup/sensors/amr_extrinsic_calibration>`.

Running the Application
-----------------------

1. SSH into the robot (:ref:`instructions<nova-carter-ssh-setup>`).

2. Make sure you have successfully connected the PS5 joystick to the robot
   (:ref:`instructions<nova-carter-joystick-setup>`).

3. Build/install the required packages and run the app:

.. run_ros_app::
    :repo_setup: <nova-carter-dev-setup>
    :repo: nova_carter
    :image: isaac/nova_carter_bringup:release_3.2-aarch64
    :package: nova_carter_bringup
    :app: perceptor.launch.py
    :asset_packages: isaac_ros_ess_models_install:isaac_ros_peoplesemseg_models_install:isaac_ros_peoplesemseg_models_install
    :asset_scripts: install_ess_models.sh:install_peoplesemsegnet_vanilla.sh:install_peoplesemsegnet_shuffleseg.sh

4. You are now able to remote control the robot with the gamepad. Follow the next
   steps to additionally visualize the sensor outputs in Foxglove.

Customizing Sensor Configurations
---------------------------------

By default, as specified in the launch file ``perceptor.launch.py``, 3 stereo
cameras available on the  `Nova Carter` (front, left, right) are used in
`Isaac Perceptor` algorithms.
You may use the ``stereo_camera_configuration`` launch argument to customize camera
configurations when running `Isaac Perceptor`.

For example, to use only front stereo camera for 3D reconstruction and visual SLAM,
you could run the following launch command:

.. code:: bash

    ros2 launch nova_carter_bringup perceptor.launch.py \
        stereo_camera_configuration:=front_configuration

For example, to use only front stereo camera for 3D reconstruction, visual SLAM
and people reconstruction, you could run the following launch command:

.. code:: bash

    ros2 launch nova_carter_bringup perceptor.launch.py \
        stereo_camera_configuration:=front_people_configuration

For a detailed description of all available configurations refer to the
:doc:`Tutorial: Stereo Camera Configurations for Isaac Perceptor </reference_workflows/isaac_perceptor/tutorial_stereo_camera_configurations>`.

Mapping and Localization
---------------------------------

In order to create maps and localize, refer to :doc:`Tutorial: Mapping and Localization with Isaac Perceptor </reference_workflows/isaac_perceptor/tutorial_mapping_and_localization>`.

Visualizing the Outputs
-----------------------

1. Make sure you complete :ref:`visualization-setup-carter`.
   This is required to visualize the `Isaac nvblox` mesh in a recommended layout configuration.

2. Open the Foxglove studio on your remote machine.

   1. If you are **not** running a configuration with people reconstruction,
      open the ``nova_carter_perceptor.json`` layout file downloaded in the previous step.

   2. If you are running a configuration with people reconstruction,
      open the ``nova_carter_perceptor_with_people.json`` layout file
      downloaded in the previous step.

3. In Foxglove, it shows a visualization of the `Nova Carter` robot and
   `Isaac nvblox` mesh visualization of surrounding environments.
   Verify that you see a visualization similar to the image below.
   In the mesh, it shows the reconstructed colored voxels,
   the computed distance map from `Isaac nvblox` outputs.
   The colored voxels are uniformly reconstructed with a resolution of 5cm.
   The rainbow color spectrum reflects the proximity of each region
   to nearest obstacles. Regions closer to obstacle surfaces are marked in warmer colors
   (red, orange), while regions further away from obstacle surfaces are marked in cooler
   colors (blue, violet).

.. figure::
    :ir_lfs:`<resources/isaac_ros_docs/repositories_and_packages/isaac_perceptor/perceptor_in_zanker.gif>`
    :width: 800px
    :alt: Foxglove visualization of the Isaac Perceptor outputs
    :align: center

4. If running a configuration with people reconstruction, in addition,
   you can see highlighted red voxels shown in the reconstructed mesh.
   It visualizes people perceived in the field of view of one or more cameras. In the default
   ``nova_carter_perceptor_with_people.json`` layout file,
   only ``/nvblox_node/dynamic_occupancy_layer`` is visualized and shown as the red voxels.

.. figure::
    :ir_lfs:`<resources/isaac_ros_docs/repositories_and_packages/isaac_perceptor/perceptor_foxglove_people_mapping.png>`
    :width: 800px
    :alt: Foxglove visualization of the `Isaac Perceptor` people reconstruction outputs
    :align: center

.. note::

   For a better visualization experience, some topics requiring a large bandwidth are not available
   to Foxglove studio. You can set ``use_foxglove_whitelist:=False`` as additional argument
   when running the app. Most likely the image stream will be fairly choppy given its large
   bandwidth. To learn more about topics published by `Isaac nvblox`, you can refer to
   :doc:`nvblox ROS messages. </repositories_and_packages/isaac_ros_nvblox/isaac_ros_nvblox/api/topics_and_services>`.
   For topics published by `Isaac visual SLAM`, you can refer to
   :doc:`cuvslam ROS messages. </repositories_and_packages/isaac_ros_visual_slam/isaac_ros_visual_slam/index>`.


Evaluating `Isaac Perceptor`
----------------------------

Follow these instructions to assert that `Isaac Perceptor` is performing as
expected.

1. `Isaac nvblox` continuously measures computation efficiency metrics and will report
   them in the terminal on request. To turn on rate output change the following
   configurations from ``false`` to ``true`` in the file
   ``$ISAAC_ROS_WS/src/isaac_perceptor/isaac_ros_perceptor_bringup/params/nvblox_perceptor.yaml``:

   .. code::

     print_rates_to_console: true
     print_delays_to_console: true

   Verify that you see metrics similar to the following:

   1. ``nvblox Rates`` reports how the frequencies of specific events happening
      in Nvblox. For the **3-camera** configuration, you are expected to see
      ``ros/depth_image_callback`` at 60Hz, and ``ros/color_image_callback``
      at 90Hz. For the **1-camera** configuration, you are expected to see
      ``ros/depth_image_callback`` at 30Hz, and ``ros/color_image_callback``
      at 30Hz.

    .. code:: bash

        nvblox Rates (in Hz)
        namespace/tag - NumSamples (Window Length) - Mean
        -----------
        ros/color                   100              13.2
        ros/depth                   100              60.4
        ros/depth_image_callback    100              60.8
        ros/color_image_callback    100              90.8
        ros/update_esdf             100              7.2
        mapper/stream_mesh          100              4.5
        ros/tick                    100              16.7
        -----------

   2. ``nvblox Delays`` reports the average compute time a node takes in second
      based on a certain number of samples. You are expected to see
      ``ros/esdf_integration`` at 100 ~ 200 ms.

    .. code:: bash

        nvblox Delays
        namespace/tag - NumSamples (Window Length) - Mean Delay (seconds)
        -----------
        ros/esdf_integration        100              0.174
        ros/depth_image_integration 100              0.152
        ros/color_image_integration 100              0.129
        ros/depth_image_callback    100              0.106
        ros/color_image_callback    100              0.068
        -----------
2. To further evaluate the quality of `Isaac Perceptor`, you can perform the following tests.

    1. Choose objects above 10cm with various heights, and place them in front of
       any camera specified in the ``stereo_camera_configuration`` launch argument.
       Vary the distance to the camera (e.g. 1m, 3m, 5m, 7m). Verify that you see the
       object in the distance map visualization.
       Additionally, you may also use the `Measure Tool` in the top panel to measure
       the distance between the object surface and the camera center.

       For instance, a pallet at the height of 10cm and a box at the height of 50cm are placed
       in front of the front stereo camera. We vary its distance to the robot
       at 0.4m, 1.0m, 3.0m, 5.0m. In the 3D view, you could see colored voxels visualizing
       the pallet and the box (illustrated by bounding boxes in green and blue). The proximity value
       measured by `Measure Tool` shows the distance between the front camera and perceived objects.
       You are expected to obtain similar proximity value with the distance you place objects at.

        .. figure::
            :ir_lfs:`<resources/isaac_ros_docs/robots/nova_developer_kit/perceptor_devkit_eval_0.4m.png>`
            :alt: Evaluating `Isaac Perceptor` voxels at 0.4m.
            :align: center

        .. figure::
            :ir_lfs:`<resources/isaac_ros_docs/robots/nova_developer_kit/perceptor_devkit_eval_1m.png>`
            :alt: Evaluating `Isaac Perceptor` voxels at 1m.
            :align: center

        .. figure::
            :ir_lfs:`<resources/isaac_ros_docs/robots/nova_developer_kit/perceptor_devkit_eval_3m.png>`
            :alt: Evaluating `Isaac Perceptor` voxels at 3m.
            :align: center

        .. figure::
            :ir_lfs:`<resources/isaac_ros_docs/robots/nova_developer_kit/perceptor_devkit_eval_5m.png>`
            :alt: Evaluating `Isaac Perceptor` voxels at 5m.
            :align: center

       You are advised to follow :doc:`Technical Details </concepts/scene_reconstruction/nvblox/technical_details>`
       to understand how scene reconstruction works.

   2. Ensure `Nova Carter` is stationary. You are expected to see no drift
      from the `Nova Carter` robot visualization with reference to the odometry frame.
      You are advised to follow :doc:`cuVSLAM </concepts/visual_slam/cuvslam/index>`
      to understand how Visual SLAM works.

Running `Isaac Perceptor` on Recorded Data
------------------------------------------

Apart from running camera-based perception on `Nova Carter` robot,
see the :doc:`Tutorial: Recording and Playing Back Data for Isaac Perceptor </reference_workflows/isaac_perceptor/tutorial_recording_and_playback>`
on how to record data on the `Nova Carter` robot and running `Isaac Perceptor` on the recorded data.
