=====================
Developer Environment
=====================

GXF Failure due to Tensor Stream Creation Failed
------------------------------------------------

When launching an Isaac ROS node on a Jetson accessed remotely with X11 forwarding, 
X events emitted by CUDA for initializing a tensor stream are captured by the remote
monitoring machine instead of the local Jetson.

Symptom
~~~~~~~

.. code:: bash

   [component_container_mt-2] 2023-09-15 14:33:11.362 ERROR /workspaces/isaac_ros-dev/src/isaac_ros_image_pipeline/isaac_ros_image_proc/gxf/tensorops/extensions/tensorops/components/TensorStream.cpp@104: tensor stream creation failed.
   [component_container_mt-2] 2023-09-15 14:33:11.363 ERROR gxf/std/entity_warden.cpp@380: Failed to initialize component 00292 (stream)
   [component_container_mt-2] 2023-09-15 14:33:11.363 ERROR gxf/core/runtime.cpp@616: Could not initialize entity 'IPOQAFVQYV_global' (E290): GXF_FAILURE
   [component_container_mt-2] 2023-09-15 14:33:11.363 ERROR gxf/std/program.cpp@205: Failed to activate entity 00290 named IPOQAFVQYV_global: GXF_FAILURE
   [component_container_mt-2] 2023-09-15 14:33:11.363 ERROR gxf/std/program.cpp@207: Deactivating...
   [component_container_mt-2] 2023-09-15 14:33:11.396 ERROR gxf/core/runtime.cpp@1227: Graph activation failed with error: GXF_FAILURE
   [component_container_mt-2] [ERROR] [1694759591.396604150] [left_decoder_node]: [NitrosContext] GxfGraphActivate Error: GXF_FAILURE
   [component_container_mt-2] [INFO] [1694759591.396906843] [right_decoder_node]: [NitrosContext] Loading application: '/workspaces/isaac_ros-dev/install/isaac_ros_h264_decoder/share/isaac_ros_h264_decoder/EZXPATGEMM.yaml'
   [component_container_mt-2] [ERROR] [1694759591.396922620] [left_decoder_node]: [NitrosNode] runGraphAsync Error: GXF_FAILURE
   [component_container_mt-2] terminate called after throwing an instance of 'std::runtime_error'
   [component_container_mt-2] what(): [NitrosNode] runGraphAsync Error: GXF_FAILURE

Solution
~~~~~~~~

Disabling X11 forwarding will prevent X events from being intercepted by the remote machine, resolving the error.

Docker ``buildx`` plugin not installed
--------------------------------------

Isaac ROS Dev scripts rely on Docker configured with the ``buildx`` plugin which is no longer installed by default.

Symptom
~~~~~~~

.. code:: console

   nvidia@ubuntu:~/workspaces/isaac_ros-dev/src/isaac_ros_common$ isaac-ros activate
   Building aarch64.ros2_humble.ros1_noetic.user base as image: isaac_ros_dev-aarch64 using key aarch64.ros2_humble.ros1_noetic.user
   Using configured docker search paths: /home/nvidia/workspaces/isaac_ros-dev/src/isaac_ros_common/scripts/../../isaac_ros_nitros_bridge/docker /home/nvidia/workspaces/isaac_ros-dev/src/isaac_ros_common/scripts/../docker
   Using base image name not specified, using ''
   Using docker context dir not specified, using Dockerfile directory
   Resolved the following Dockerfiles for target image: aarch64.ros2_humble.ros1_noetic.user
   /home/nvidia/workspaces/isaac_ros-dev/src/isaac_ros_common/scripts/../docker/Dockerfile.user
   /home/nvidia/workspaces/isaac_ros-dev/src/isaac_ros_common/scripts/../../isaac_ros_nitros_bridge/docker/Dockerfile.ros1_noetic
   /home/nvidia/workspaces/isaac_ros-dev/src/isaac_ros_common/scripts/../docker/Dockerfile.aarch64.ros2_humble
   Building /home/nvidia/workspaces/isaac_ros-dev/src/isaac_ros_common/scripts/../docker/Dockerfile.aarch64.ros2_humble as image: aarch64-ros2_humble-image with base:
   ERROR: BuildKit is enabled but the buildx component is missing or broken.
         Install the buildx component to build images with BuildKit:
         https://docs.docker.com/go/buildx/
   Failed to build base image: isaac_ros_dev-aarch64, aborting.   

Solution
~~~~~~~~

   .. include:: /_snippets/docker_buildx.rst



Repository cloned without pulling ``git-lfs`` files
---------------------------------------------------

Symptom
~~~~~~~

You fail building some ``isaac_ros`` packages, or some image files in the
repository are empty.

Solution
~~~~~~~~

Many Isaac ROS repositories are configured with Git LFS. For example,
``isaac_ros_visual_slam`` repository is configured with Git LFS to accommodate
big rosbag files for test.

To check if some of the LFS files are not resolved, you can first list
all the files that are supposed to be managed by Git LFS.

.. code:: bash

   cd ${ISAAC_ROS_WS}/src/isaac_ros_common/ && \
     git lfs ls-files -s

It should give something like this.

   Example of output of ``git lfs ls-files -s``:

   .. code:: bash

      cfbf5fbcee - resources/Isaac_sim_app_launcher.png (68 KB)
      448d244a3e - resources/Isaac_sim_app_terminal.png (57 KB)
      750f7de371 - resources/graph_read-write-speed.png (77 KB)
      c29c6d6a3a - resources/isaac_ros_common_tools.png (30 KB)
      87e38eea78 - resources/isaac_sim_initial_screen.png (265 KB)
      e2292bc330 - resources/isaac_sim_nucleus_setup.png (221 KB)
      ea8b297681 - resources/isaac_sim_ros_bridge.png (300 KB)

Now check the actual file size of those files.

.. code:: bash

   git lfs ls-files -s | awk '{ print $3 }' | xargs ls -l

..

   Example of output of
   ``git lfs ls-files -s | awk '{ print $3 }' | xargs ls -l``:

   .. code:: bash 
      
      -rw-rw-r- 1 jetson jetson 130 Mar 22 08:56 resources/graph_read-write-speed.png
      -rw-rw-r- 1 jetson jetson 130 Mar 22 08:56 resources/isaac_ros_common_tools.png 
      -rw-rw-r- 1 jetson jetson 130 Mar 22 08:56 resources/Isaac_sim_app_launcher.png 
      -rw-rw-r- 1 jetson jetson 130 Mar 22 08:56 resources/Isaac_sim_app_terminal.png
      -rw-rw-r- 1 jetson jetson 131 Mar 22 08:56 resources/isaac_sim_initial_screen.png 
      -rw-rw-r- 1 jetson jetson 131 Mar 22 08:56 resources/isaac_sim_nucleus_setup.png 
      -rw-rw-r- 1 jetson jetson 131 Mar 22 08:56 resources/isaac_sim_ros_bridge.png

In above example, all the file sizes are 130byte or 131byte, much
smaller than what was advertised by ``git lfs ls-files -s``. This shows
that those LFS files are not really pulled.

Optionally, if you know a directory that should have a large LFS file,
you can check the directory size.

.. code:: bash

   du -hs ${ISAAC_ROS_WS}/src/isaac_ros_visual_slam/isaac_ros_visual_slam/test/test_cases/rosbags/
   20K     /home/username/workspaces/isaac_ros-dev/src/isaac_ros_visual_slam/isaac_ros_visual_slam/test/test_cases/rosbags/

If any of above is the case, you can manually pull the LFS files, after
installing ``git lfs``.

.. code:: bash

   cd ${ISAAC_ROS_WS}/src/isaac_ros_common && \
     git lfs pull

or

.. code:: bash

   cd ${ISAAC_ROS_WS}/src/isaac_ros_visual_slam && \
     git lfs pull -X "" -I isaac_ros_visual_slam/test/test_cases/rosbags/

X11-forward issue
-----------------


Symptom
~~~~~~~

When you run X11 window application inside the Isaac ROS container, you
see an error message:

   “X11 connection rejected because of wrong authentication.”


Solution
~~~~~~~~

Go back to your docker host, and re SSH-login from your remote machine
with ``ssh -X`` option.

.. code:: bash

   ssh -X ${USER}@${IP}

Check if you see an error message like this.

.. code:: bash

   /usr/bin/xauth:  /home/jetson/.Xauthority not writable, changes will be ignored
   /usr/bin/xauth:  /home/jetson/.Xauthority not writable, changes ignored

If so, delete the ``~/.Xauthority`` directory.

.. code:: bash

   sudo rm -rf ~/.Xauthority/

Log out and re-login with ``ssh -X``.

Check if you have ``.Xauthority`` file with your user as owner.

.. code:: bash

   ~$ ll ~/.Xauthority 
   -rw------- 1 jetson jetson 61 Mar 22 14:08 /home/jetson/.Xauthority

Now, you should be able to achieve X11 forwarding. Try with ``xeyes`` on
your docker host.

Then, launch your container and try running the app that requires X11
forwarding.

ROS 2 Domain ID Collision
-------------------------


Symptom
~~~~~~~

-  You see ROS 2 topics that your system is not (yet) publishing
-  ROS 2 topics don't go away even after you stop the node that you
   think is publishing


Solution
~~~~~~~~

By default, all ROS 2 nodes use domain ID 0, so it is possible you are
seeing topics from other machines on the network
(`docs.ros.org <https://docs.ros.org/en/:ir_ros_distro:/Concepts/About-Domain-ID.html>`__).

**Solution 1**:

You can declare ``ROS_DOMAIN_ID`` on every bash instance inside the
Isaac ROS container.

.. code:: bash

   ROS_DOMAIN_ID=42

**Solution 2**:

You declare the ``ROS_DOMAIN_ID`` on the host once, and update the Isaac ROS CLI configuration to carry the environment variable.

.. code:: bash

   echo '-e ROS_DOMAIN_ID' > ~/.isaac_ros_dev-dockerargs

No ``/opt/ros/humble/install`` directory in Isaac ROS environment
-----------------------------------------------------------------

Symptom
~~~~~~~

You expect the ``/opt/ros/humble/install`` directory to exist inside the
Isaac ROS environment, but this directory does not exist.

This is the result of a change made in the Isaac ROS 2.1 release in November 2023. 

In Isaac ROS 2.1, all base ROS 2 Humble packages are installed using pre-built Debian packages produced by the NVIDIA ROS 2 Buildfarm.
Previously, these base packages were built from source in a customized global underlay workspace rooted at ``/opt/ros/humble/``.

Since the base packages are no longer built from source inline within the image, the ``./install`` subfolder of ``/opt/ros/humble`` is never created.

Solution
~~~~~~~~

**Solution 1**:

Move any user-specific ROS 2 packages out of the global underlay workspace to an appropriate local overlay workspace, as recommended by the `official ROS documentation <https://docs.ros.org/en/foxy/Tutorials/Beginner-Client-Libraries/Creating-A-Workspace/Creating-A-Workspace.html#background>`__. 

Then, build and source this overlay workspace normally.

**Solution 2**:

Carefully recreate the customized global underlay workspace by following the steps used in Isaac ROS 2.0 and older releases:

.. code:: docker

   # EXAMPLE: Install OSRF negotiated ROS 2 package from source

   ENV ROS_DISTRO=humble
   ENV ROS_ROOT=/opt/ros/${ROS_DISTRO}

   RUN apt-get update && mkdir -p ${ROS_ROOT}/src && cd ${ROS_ROOT}/src \
      && git clone https://github.com/osrf/negotiated && cd negotiated && git checkout master \
      && source ${ROS_ROOT}/setup.bash \
      && cd negotiated_interfaces && bloom-generate rosdebian && fakeroot debian/rules binary \
      && cd ../ && apt-get install -y ./*.deb && rm ./*.deb \
      && cd negotiated && bloom-generate rosdebian && fakeroot debian/rules binary \
      && cd ../ && apt-get install -y ./*.deb && rm ./*.deb \
   && rm -rf /var/lib/apt/lists/* \
   && apt-get clean

This process involves overriding the behavior of ``bloom-generate`` to force it to produce and install the Debian packages inline.

``isaac-ros activate`` fails to build image with Docker certificate errors
--------------------------------------------------------------------------

Symptom
~~~~~~~

Docker build fails building base image with error messages with the following form:

.. code:: bash

   tls: failed to verify certificate: x509: certificate has expired or is not yet valid

More context:

.. code:: docker

   [+] Building 0.2s (2/2) FINISHED                                                                                                                                                                                 docker:default
   => [internal] load build definition from Dockerfile.aarch64                                                                                                                                                               0.0s
   => => transferring dockerfile: 8.45kB                                                                                                                                                                                     0.0s
   => ERROR [internal] load metadata for nvcr.io/nvidia/nvidia-aarch64-base:ubuntu22.04                                                                                                                                     0.1s
   ------
   > [internal] load metadata for nvcr.io/nvidia/nvidia-aarch64-base:ubuntu22.04:
   ------
   Dockerfile.aarch64:11
   --------------------
      9 |     # Docker file for aarch64 based Jetson device
   10 |     ARG BASE_IMAGE="nvcr.io/nvidia/nvidia-aarch64-base:ubuntu22.04"
   11 | >>> FROM ${BASE_IMAGE}
   12 |
   13 |     # Store list of packages (must be first)
   --------------------
   ERROR: failed to solve: nvcr.io/nvidia/nvidia-aarch64-base:ubuntu22.04: failed to resolve source metadata for nvcr.io/nvidia/nvidia-aarch64-base:ubuntu22.04: failed to do request: Head "https://nvcr.io/v2/nvidia/nvidia-aarch64-base/manifests/ubuntu22.04";: tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 1969-12-31T17:10:53-08:00 is before 2024-04-24T00:00:00Z
   /mnt/nova_ssd/workspaces/isaac_ros-dev/src/isaac_ros_common


Solution
~~~~~~~~

Restarting the Docker daemon should resolve the issue:

.. code:: bash

   sudo systemctl restart docker

Failed to load a NITROS enabled nodes with intraprocess communication enabled
-----------------------------------------------------------------------------

When launching a NITROS-enabled ROS node with explicitly setting ``use_intra_process_comms`` to ``True``, the following error can be seen.

Symptom
~~~~~~~

.. code:: bash

   [component_container-1] [ERROR] [1715506914.017761602] [camera_1.camera_1_container]: Component constructor threw an exception: intraprocess communication allowed only with volatile durability
   [ERROR] [launch_ros.actions.load_composable_nodes]: Failed to load node 'visual_slam_node' of type 'nvidia::isaac_ros::visual_slam::VisualSlamNode' in container '/camera_1/camera_1_container': Component constructor threw an exception: intraprocess communication allowed only with volatile durability

Solution
~~~~~~~~

NITROS nodes already have intraprocess communication enabled by default, with an exception of few publishers and subscribers that does not participate in data transfer. Disabling intraprocess communication by setting the parameter to ``False`` should resolve the issue and it does not hamper the performance.

.. _spark-rosdep-version-conflicts:

``rosdep install`` fails on DGX Spark with package downgrade error
------------------------------------------------------------------

On DGX Spark platforms (bare-metal and virtual environment workflows only),
``rosdep install`` may fail because the system has newer versions of some packages than
those pinned by Isaac ROS. The Isaac ROS apt preference files in ``/etc/apt/preferences.d/``
pin packages at priority 1001 to enforce tested versions. When ``apt`` needs to downgrade
any package to satisfy these pins, it rejects the **entire** transaction—meaning even
unrelated Isaac ROS packages fail to install.

Symptom
~~~~~~~

.. code:: bash

   The following packages will be DOWNGRADED:
     cuda-toolkit-13-config-common cuda-toolkit-config-common
   E: Packages were downgraded and -y was used without --allow-downgrades.

You may also see errors like the following when trying to launch Isaac ROS nodes, because
the packages were never installed:

.. code:: bash

   Package 'nvblox_examples_bringup' not found: "package 'nvblox_examples_bringup' not found, searching: ['/opt/ros/jazzy']"

Solution
~~~~~~~~

**Option 1: Allow downgrades**

If this is a dedicated Isaac ROS system, you can configure ``apt`` to allow package
downgrades system-wide. This ensures the exact package versions tested with Isaac ROS
are installed, but **may break other software** on the system that depends on the newer
versions.

.. code:: bash

   echo 'APT::Get::allow-downgrades "true";' | sudo tee /etc/apt/apt.conf.d/99allow-downgrades

After setting this configuration, re-run the ``rosdep install`` command. You can remove
this configuration afterward:

.. code:: bash

   sudo rm /etc/apt/apt.conf.d/99allow-downgrades

**Option 2: Disable version pinning for conflicting packages**

If you have other projects on the system that depend on the newer package versions,
you can disable the Isaac ROS apt preference files that cause the conflict. This allows
``apt`` to keep the DGX OS package versions instead of forcing a downgrade. These versions
are **not tested** with Isaac ROS and **may not work correctly**.

The Isaac ROS apt preference files follow the ``isaac-ros-*.pref`` naming pattern in
``/etc/apt/preferences.d/``. Rename the relevant ones to disable them:

.. code:: bash

   # For example, if CUDA packages are conflicting:
   sudo mv /etc/apt/preferences.d/isaac-ros-cuda-13-0.pref /etc/apt/preferences.d/isaac-ros-cuda-13-0.pref.bak

After renaming, re-run the ``rosdep install`` command.

To restore the original pinning later:

.. code:: bash

   sudo mv /etc/apt/preferences.d/isaac-ros-cuda-13-0.pref.bak /etc/apt/preferences.d/isaac-ros-cuda-13-0.pref

H.264 Encode/Decode May Fail on DGX Spark
------------------------------------------

Symptom
~~~~~~~

On DGX Spark, the H.264 encoder and decoder nodes may fail to launch with the
following error:

.. code:: bash

   Error: libnvbufsurface.so.1.0.0: cannot open shared object file: No such file or directory

Solution
~~~~~~~~

This will be fixed in a future release. As a workaround, set the following
environment variable before launching the node:

.. code:: bash

   export LD_LIBRARY_PATH="/usr/lib/aarch64-linux-gnu/nvidia:${LD_LIBRARY_PATH}"

Linker Error ``foo.so: file format not recognized; treating as linker script``
------------------------------------------------------------------------------


Symptom
~~~~~~~

When attempting to build an Isaac ROS package by running
``colcon build``, the linker complains with an error that resembles the
following:

.. code:: bash

   --- stderr: isaac_ros_nitros                                                                                      
   /usr/bin/ld:/workspaces/isaac_ros-dev/src/isaac_ros_nitros/isaac_ros_nitros/gxf/lib/gxf_x86_64_cuda_11_7/core/libgxf_core.so: file format not recognized; treating as linker script
   /usr/bin/ld:/workspaces/isaac_ros-dev/src/isaac_ros_nitros/isaac_ros_nitros/gxf/lib/gxf_x86_64_cuda_11_7/core/libgxf_core.so:1: syntax error
   collect2: error: ld returned 1 exit status
   make[2]: *** [CMakeFiles/isaac_ros_nitros.dir/build.make:311: libisaac_ros_nitros.so] Error 1
   make[1]: *** [CMakeFiles/Makefile2:139: CMakeFiles/isaac_ros_nitros.dir/all] Error 2
   make: *** [Makefile:146: all] Error 2
   ---
   Failed   <<< isaac_ros_nitros [0.52s, exited with code 2]


Solution
~~~~~~~~

This error typically arises when the local copy of a ``.so`` binary on
your machine contains only the Git LFS pointer, instead of the actual
binary contents.

.. note::

   If you're encountering this error, you may have forgotten
   to install and perform one-time setup for Git LFS on your machine.

To correct this, navigate to the Isaac ROS package that is failing to
build and re-pull the binary files using Git LFS.

For example, if ``isaac_ros_nitros`` is failing to build as shown above:

1. Navigate to the package:

   .. code:: bash

      cd ${ISAAC_ROS_WS}/src/isaac_ros_nitros

2. Pull from Git LFS:

   .. code:: bash

      git lfs pull

3. Return to the workspace and run the build command again:

   .. code:: bash

      cd ${ISAAC_ROS_WS} && colcon build --symlink-install