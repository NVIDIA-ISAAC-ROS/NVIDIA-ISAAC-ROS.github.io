==============
|package_name|
==============

:ir_github:`<isaac_ros_pose_estimation> <isaac_ros_dope>`

Quickstart
----------

.. warning::

   This package requires a model preparation process that differs significantly from those of other Isaac ROS packages.
   Running this quickstart requires a conversion step that must be performed on an ``x86_64`` system.
   It is not possible to complete this tutorial using only a Jetson device.

   To run on Jetson, first complete the tutorial on ``x86_64``.
   Then, manually copy the converted model from the ``x86_64`` host to the Jetson and repeat the tutorial on the Jetson.

Set Up Development Environment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /_snippets/set_up_dev_env.rst

Download Quickstart Assets
~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Download quickstart data from NGC:

   :ir_assets:`<isaac_ros_dope> <quickstart.tar.gz>`

#. Download the ``Ketchup.pth`` DOPE model from the official `DOPE GitHub <https://github.com/NVlabs/Deep_Object_Pose>`__ repository's
   model collection available `here <https://drive.google.com/open?id=1DfoA3m_Bm0fW8tOWXGVxi4ETlLEAgmcg>`__.

   Move this file to ``${ISAAC_ROS_WS}/isaac_ros_assets/models/dope``.

   For example, if the model was downloaded to ``~/Downloads``:

   .. code:: bash

      mkdir -p ${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/ && \
         mv ~/Downloads/Ketchup.pth ${ISAAC_ROS_WS}/isaac_ros_assets/models/dope

Build |package_name|
~~~~~~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: Binary Package

      #. Activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Install the prebuilt Debian package:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-dope

   .. tab:: Build from Source

      #. Install Git LFS:

         .. code:: bash

            sudo apt-get install -y git-lfs && git lfs install

      #. Clone this repository under ``${ISAAC_ROS_WS}/src``:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/src && \
               git clone :ir_clone:`<isaac_ros_pose_estimation>`

      #. Activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Use ``rosdep`` to install the package's dependencies:

         :ir_apt:

         .. code:: bash

            rosdep update && rosdep install --from-paths ${ISAAC_ROS_WS}/src/isaac_ros_pose_estimation/isaac_ros_dope --ignore-src -y

      #. Build the package from source:

         .. code:: bash

            cd ${ISAAC_ROS_WS} && \
               colcon build --symlink-install --packages-up-to isaac_ros_dope --base-paths ${ISAAC_ROS_WS}/src/isaac_ros_pose_estimation/isaac_ros_dope

      #. Source the ROS workspace:

         .. note::

            Make sure to repeat this step in **every** terminal created inside the Isaac ROS environment.

            Because this package was built from source, the enclosing workspace must be sourced for ROS to be able to find the package's contents.

         .. code:: bash

            source install/setup.bash

Run Launch File
~~~~~~~~~~~~~~~

#. Continuing inside the Isaac ROS environment, convert the PyTorch model (``.pth``) to a general ONNX model (``.onnx``):

   .. warning::

         This step must be performed on ``x86_64``. If you intend to run the model on a Jetson,
         you must first convert the model on an ``x86_64`` system, and then copy the output file to the
         corresponding location on the Jetson (``${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/Ketchup.onnx``)

   First install ``torchvision``:

   .. code:: bash

      pip3 install --break-system-packages --no-deps --index-url https://download.pytorch.org/whl/cu130 torchvision==0.24.0+cu130 || \
      pip3 install --break-system-packages --no-deps --index-url https://download.pytorch.org/whl/cu130 torchvision==0.24.0

   Now convert the model:

   .. code:: bash

      ros2 run isaac_ros_dope dope_converter.py --format onnx \
         --input ${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/Ketchup.pth --output ${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/Ketchup.onnx --row 720 --col 1280

.. tabs::

   .. group-tab:: Rosbag

      #. Continuing inside the Isaac ROS environment, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-examples

      #. Run the following launch file to spin up a demo of this package using the quickstart rosbag:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=dope interface_specs_file:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_dope/quickstart_interface_specs.json \
               model_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/Ketchup.onnx engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/Ketchup.plan

      #. Open a **second** terminal inside the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Run the rosbag file to simulate an image stream:

         .. code:: bash

            ros2 bag play -l ${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_dope/quickstart.bag

   .. group-tab:: RealSense Camera

      #. Ensure that you have already set up your RealSense camera using the :doc:`RealSense setup tutorial </getting_started/sensors/realsense_setup>`. If you have not, please set up the sensor and then restart this quickstart from the beginning.

      #. Continuing inside the Isaac ROS environment, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-examples ros-:ir_ros_distro:-isaac-ros-realsense

      #. Run the following launch file to spin up a demo of this package using a RealSense camera:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=realsense_mono_rect,dope \
              model_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/Ketchup.onnx engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/dope/Ketchup.plan

Visualize Results
~~~~~~~~~~~~~~~~~

#. Open a **new** terminal inside the Isaac ROS environment:

   .. code:: bash

      isaac-ros activate

#. Install RViz and the ``vision_msgs`` RViz plugin:

   .. code:: bash

      sudo apt-get install -y ros-:ir_ros_distro:-rviz2 ros-:ir_ros_distro:-vision-msgs-rviz-plugins
      source /opt/ros/:ir_ros_distro:/setup.bash

#. Visualize the detection3d array in RViz2:

   .. code:: bash

      rviz2

   .. tabs::

      .. group-tab:: Rosbag

         Make sure to update the ``Fixed Frame`` to ``tf_camera``.

      .. group-tab:: RealSense Camera

         Make sure to update the ``Fixed Frame`` to ``camera_color_optical_frame``.

#. Then click on the ``Add`` button, select ``By display type`` and choose
   ``Detection3DArray`` under ``vision_msgs_rviz_plugins``. Expand the Detection3DArray
   display and change the topic to ``/detections``. Check the ``Only Edge`` option.
   Then click on the ``Add`` button again and select ``By Topic``.
   Under  ``/dope_encoder``, expand the ``/resize`` drop-down, select ``/image``, and click
   the ``Camera`` option to see the image with the bounding box over detected objects.
   Refer to the picture below.

   .. figure:: :ir_lfs:`<resources/isaac_ros_docs/repositories_and_packages/isaac_ros_pose_estimation/isaac_ros_dope/dope_rviz1.png>`
      :width: 600px
      :align: center

   .. figure:: :ir_lfs:`<resources/isaac_ros_docs/repositories_and_packages/isaac_ros_pose_estimation/isaac_ros_dope/dope_rviz2.png>`
      :width: 600px
      :align: center

Try More Examples
-----------------

To continue your exploration, check out the following suggested examples:

.. toctree::
   :maxdepth: 1

   Tutorial with Triton </concepts/pose_estimation/dope/tutorial_triton>

   Tutorial with Custom Model </concepts/pose_estimation/dope/tutorial_custom_model>

   Tutorial with Custom Size </concepts/pose_estimation/dope/tutorial_custom_size>

   Verifying DOPE Accuracy </concepts/pose_estimation/dope/dope_verification>

.. note::

   For best results, always crop or resize input images to the same dimensions your DOPE model is expecting.

Use Different Models
--------------------

Click :doc:`here </concepts/dnn_inference/model_preparation>` for more information on how to use NGC models.

Alternatively, consult the ``DOPE`` model repository to try other models.

.. list-table::
   :header-rows: 1

   * - Model Name
     - Use Case
   * - `DOPE <https://drive.google.com/open?id=1DfoA3m_Bm0fW8tOWXGVxi4ETlLEAgmcg>`__
     - The DOPE model repository. This should be used if ``isaac_ros_dope`` is used

Troubleshooting
---------------

Isaac ROS Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with Isaac ROS, please check :doc:`here </troubleshooting/index>`.

Deep Learning Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with using DNN models, please check :doc:`here </troubleshooting/deep_learning>`.

API
----

Usage
~~~~~~

Two launch files are provided to use this package. The first launch file launches ``isaac_ros_tensor_rt``, whereas the other one uses ``isaac_ros_triton``, along with
the necessary components to perform encoding on images and decoding of the ``DOPE`` network's output.

.. warning::
   For your specific application, these launch files may need to be modified. Please consult the available components to see
   the configurable parameters.

.. list-table::
   :header-rows: 1

   * - Launch File
     - Components Used
   * - ``isaac_ros_dope_tensor_rt.launch.py``
     - :doc:`DnnImageEncoderNode </repositories_and_packages/isaac_ros_dnn_inference/isaac_ros_dnn_image_encoder/index>`, :doc:`TensorRTNode </repositories_and_packages/isaac_ros_dnn_inference/isaac_ros_tensor_rt/index>`, :doc:`DopeDecoderNode </repositories_and_packages/isaac_ros_pose_estimation/isaac_ros_dope/index>`
   * - ``isaac_ros_dope_triton.launch.py``
     - :doc:`DnnImageEncoderNode </repositories_and_packages/isaac_ros_dnn_inference/isaac_ros_dnn_image_encoder/index>`, :doc:`TritonNode </repositories_and_packages/isaac_ros_dnn_inference/isaac_ros_triton/index>`, :doc:`DopeDecoderNode </repositories_and_packages/isaac_ros_pose_estimation/isaac_ros_dope/index>`

.. warning::

   There is also a ``config`` file that should be modified in
   ``isaac_ros_dope/config/dope_config.yaml``.

DopeDecoderNode
~~~~~~~~~~~~~~~

ROS Parameters
^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Parameter
     - Type
     - Default
     - Description
   * - ``configuration_file``
     - ``string``
     - ``dope_config.yaml``
     - The name of the configuration file to parse. Note: The node will look for that file name under ``isaac_ros_dope/config``
   * - ``object_name``
     - ``string``
     - ``Ketchup``
     - The object class the DOPE network is detecting and the DOPE decoder is interpreting. This name should be listed in the configuration file along with its corresponding cuboid dimensions.
   * - ``enable_tf_publishing``
     - ``bool``
     - ``false``
     - Whether the DOPE Decoder Node will broadcast poses to the TF Tree.
   * - ``map_peak_threshold``
     - ``double``
     - ``0.1``
     - The minimum value of a peak in a DOPE belief map.
   * - ``rotation_y_axis``
     - ``double``
     - ``0.0``
     - Rotation along Y axis in degrees
   * - ``rotation_x_axis``
     - ``double``
     - ``0.0``
     - Rotation along X axis in degrees
   * - ``rotation_z_axis``
     - ``double``
     - ``0.0``
     - Rotation along Z axis in degrees

Configuration File
^^^^^^^^^^^^^^^^^^

The DOPE configuration file, which can be found at ``isaac_ros_dope/config/dope_config.yaml`` may need to modified. Specifically, you will need to specify an object type in the ``DopeDecoderNode`` that is listed in the ``dope_config.yaml`` file. The DOPE decoder node will pick the right parameters to transform the belief maps from the inference node to object poses.

.. note::

   The ``object_name`` should correspond to one of the objects listed in the DOPE configuration file, with the corresponding model used.

.. note::
   The parameters for rotation can be used to specify a rotation of the pose output by the network, it is useful when one would like to align detections between Foundation Pose and Dope. For example, for the soup can asset, the rotation would need to done along the y axis by 90 degrees. All rotation values here are in degrees. The rotation is performed in a ZYX sequence.

ROS Topics Subscribed
^^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``belief_map_array``
     - :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`
     - The tensor that represents the belief maps, which are outputs from the DOPE network.
   * - ``camera_info``
     - `sensor_msgs/CameraInfo <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/CameraInfo.msg>`__
     - The input image camera_info.

ROS Topics Published
^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``dope/detections``
     - `vision_msgs/Detection3DArray <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection3DArray.msg>`__
     - An array of detections found by DOPE network and outputted by DOPE Decoder Node. Each detection specifies pose and bounding box dimensions.

.. warning::

   The DOPE network outputs one pose for each instance of an object.
   As such, the ``ObjectHypothesis[] results`` attribute for each element
   in the ``Detection3DArray`` has only one element that includes
   the object pose, but does not specify the ```score``.

.. |package_name| replace:: ``isaac_ros_dope``
