==============
|package_name|
==============

:ir_github:`<isaac_ros_object_detection> <isaac_ros_yolov8>`

Quickstart
----------

Set Up Development Environment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /_snippets/set_up_dev_env.rst

Download Quickstart Assets
~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Download quickstart data from NGC:

   :ir_assets:`<isaac_ros_yolov8> <quickstart.tar.gz>`

#. Activate the Isaac ROS Environment:

   .. code:: bash

      isaac-ros activate

#. Create a temporary virtual environment and enter it. For this example, it will be called ``yolo``:

   .. code:: bash

      python3 -m venv yolo && source yolo/bin/activate

#. Download the model of your choice from `Ultralytics YOLOv8 <https://github.com/ultralytics/ultralytics#models>`__.
   For this example, we use `YOLOv8s <https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt>`__.

   .. code:: bash

      wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt

#. Convert the PyTorch model (``.pt``) to a general ONNX model (``.onnx``). Export to `ONNX <https://onnx.ai/>`__ following instructions given below or `here <https://docs.ultralytics.com/modes/export/>`__.

   This can be done by first installing ``ultralytics`` and ``onnx`` via pip:

   .. code:: bash

      pip3 install ultralytics
      pip3 install onnx

   .. code:: bash

      sudo apt update
      sudo apt install -y libgl1

   Afterwards, convert the model from a ``.pt`` file to a ``.onnx`` model using ``ultralytics``. This can be done by running:

   .. code:: bash

      python3

   Then within ``python3``, export the model to ``.onnx`` format:

   .. code:: python

      >> from ultralytics import YOLO
      >> model = YOLO('yolov8s.pt')
      >> model.export(format='onnx')

   Then exit the interactive Python shell, and deactivate the virtual environment:

   .. code:: bash

      deactivate

   Copy the generated ``.onnx`` model into the designated location for Isaac ROS (``${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8``):

   .. code:: bash

      mkdir -p ${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8
      cp yolov8s.onnx ${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8

   Exit the Isaac ROS environment:

   .. code:: bash

      exit

Build |package_name|
~~~~~~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: Binary Package

      #. Activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Install the prebuilt Debian package:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-yolov8 ros-:ir_ros_distro:-isaac-ros-dnn-image-encoder ros-:ir_ros_distro:-isaac-ros-tensor-rt

   .. tab:: Build from Source

      #. Install Git LFS:

         .. code:: bash

            sudo apt-get install -y git-lfs && git lfs install

      #. Clone this repository under ``${ISAAC_ROS_WS}/src``:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/src && \
               git clone :ir_clone:`<isaac_ros_object_detection>`

      #. Activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Use ``rosdep`` to install the package's dependencies:

         :ir_apt:

         .. code:: bash

            rosdep update && rosdep install --from-paths ${ISAAC_ROS_WS}/src/isaac_ros_object_detection/isaac_ros_yolov8 --ignore-src -y

      #. Build the package from source:

         .. code:: bash

            cd ${ISAAC_ROS_WS} && \
               colcon build --symlink-install --packages-up-to isaac_ros_yolov8 --base-paths ${ISAAC_ROS_WS}/src/isaac_ros_object_detection/isaac_ros_yolov8

      #. Source the ROS workspace:

         .. note::

            Make sure to repeat this step in **every** terminal created inside the Isaac ROS environment.

            Since this package was built from source, the enclosing workspace must be sourced for ROS to be able to find the package's contents.

         .. code:: bash

            source install/setup.bash

Convert ONNX Model to TensorRT
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Convert the ONNX model to a TensorRT engine file (``.plan``) using `trtexec <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec>`__.
   Arguments can be specified for FP16 quantization during this step. You can use `netron <https://netron.app/>`__ to
   visualize the ONNX model and note input and output names and dimensions.

   .. code:: bash

      /usr/src/tensorrt/bin/trtexec --onnx=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.onnx --saveEngine=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan

Run Launch File
~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: Rosbag

      #. Continuing inside the Isaac ROS environment, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-examples

      #. Run the following launch file to spin up a demo of this package using the quickstart rosbag:

         .. code:: bash

            cd /workspaces/isaac_ros-dev && \
            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=yolov8 interface_specs_file:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_yolov8/quickstart_interface_specs.json \
               engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan

      #. Open a **second** terminal inside the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Run the rosbag file to simulate an image stream:

         .. code:: bash

            ros2 bag play -l ${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_yolov8/quickstart.bag

   .. tab:: RealSense Camera

      #. Ensure that you have already set up your RealSense camera using the :doc:`RealSense setup tutorial </getting_started/sensors/realsense_setup>`. If you have not, please set up the sensor and then restart this quickstart from the beginning.

      #. Continuing inside the Isaac ROS environment, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-examples ros-:ir_ros_distro:-isaac-ros-realsense

      #. Run the following launch file to spin up a demo of this package using a RealSense camera:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=realsense_mono_rect,yolov8 \
               engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan

   .. tab:: ZED Camera

      #. Ensure that you have already set up your ZED camera using :doc:`ZED setup tutorial </getting_started/sensors/zed_setup>`.

      #. Continuing inside the Isaac ROS environment, install dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-examples ros-:ir_ros_distro:-isaac-ros-stereo-image-proc ros-:ir_ros_distro:-isaac-ros-zed

      #. Run the following launch file to spin up a demo of this package using a ZED Camera:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py \
            launch_fragments:=zed_mono_rect,yolov8 \
            engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan \
            interface_specs_file:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_yolov8/zed2_quickstart_interface_specs.json

         .. note::

            If you are using the `ZED X` series, replace  `zed2_quickstart_interface_specs.json` with `zedx_quickstart_interface_specs.json` in the above command.


Visualize Results
~~~~~~~~~~~~~~~~~

#. Open a **new** terminal inside the Isaac ROS environment:

   .. code:: bash

      isaac-ros activate

#. Run the YOLOv8 visualization script:

   .. code:: bash

      ros2 run isaac_ros_yolov8 isaac_ros_yolov8_visualizer.py

#. Open **another** terminal inside the Isaac ROS environment:

   .. code:: bash

      isaac-ros activate

#. Install ``rqt_image_view``:

   .. code:: bash

      sudo apt-get install -y ros-:ir_ros_distro:-rqt-image-view

#. Visualize and validate the output of the package with ``rqt_image_view``:

   .. code:: bash

      ros2 run rqt_image_view rqt_image_view /yolov8_processed_image

   Your output should look like this:

   .. figure:: :ir_lfs:`<resources/isaac_ros_docs/repositories_and_packages/isaac_ros_object_detection/isaac_ros_yolov8/people-cycling-results.png>`
      :alt: RQT showing detection of people cycling and bikes
      :align: center

Troubleshooting
---------------

Isaac ROS Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with Isaac ROS, please check :doc:`here </troubleshooting/index>`.

Deep Learning Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with using DNN models, please check :doc:`here </troubleshooting/deep_learning>`.

API
----

Usage
-----

.. code:: bash

   ros2 launch isaac_ros_yolov8 isaac_ros_yolov8_visualize.launch.py model_file_path:=<model_file_path> engine_file_path:=<engine_file_path> input_binding_names:=<input_binding_names> output_binding_names:=<output_binding_names> network_image_width:=<network_image_width> network_image_height:=<network_image_height> force_engine_update:=<force_engine_update> image_mean:=<image_mean> image_stddev:=<image_stddev> confidence_threshold:=<confidence_threshold> nms_threshold:=<nms_threshold>

Yolov8DecoderNode
~~~~~~~~~~~~~~~~~

ROS Parameters
^^^^^^^^^^^^^^

=============================== ============ ============================= ===========================================================================================================================================================================================================================
ROS Parameter                   Type         Default                       Description
=============================== ============ ============================= ===========================================================================================================================================================================================================================
``tensor_name``                 ``string``   ``"output_tensor"``           Name of the inferred output tensor published by the Managed NITROS Publisher. The decoder uses this name to get the output tensor.
``confidence_threshold``        ``float``    ``0.25``                      Detection confidence threshold. Used to filter candidate detections during Non-Maximum Suppression (NMS).
``nms_threshold``               ``float``    ``0.45``                      NMS IOU threshold.
=============================== ============ ============================= ===========================================================================================================================================================================================================================


ROS Topics Subscribed
^^^^^^^^^^^^^^^^^^^^^

============== ==================================================================================================================================================================== ===================================================================================================
ROS Topic      Interface                                                                                                                                                            Description
============== ==================================================================================================================================================================== ===================================================================================================
``tensor_sub`` :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`                                      Tensor list from the managed NITROS subscriber that represents the inferred aligned bounding boxes.
============== ==================================================================================================================================================================== ===================================================================================================

ROS Topics Published
^^^^^^^^^^^^^^^^^^^^

======================== =============================================================================================================================== ==================================================
ROS Topic                Interface                                                                                                                       Description
======================== =============================================================================================================================== ==================================================
``detections_output``    `vision_msgs/Detection2DArray <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection2DArray.msg>`__ Aligned image bounding boxes with detection class.
======================== =============================================================================================================================== ==================================================

.. |package_name| replace:: ``isaac_ros_yolov8``
