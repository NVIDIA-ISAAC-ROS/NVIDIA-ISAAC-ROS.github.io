==============
|package_name|
==============

:ir_github:`<isaac_ros_pose_estimation> <isaac_ros_foundationpose>`

.. important::

   The peak GPU memory usage for the ``isaac_ros_foundationpose`` (FP32) pipeline in this tutorial is approximately 7 GB. Therefore, we recommend using a GPU with at least 8 GB of memory.

Quickstart
----------

Set Up Development Environment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /_snippets/set_up_dev_env.rst

Download Quickstart Assets
~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Download quickstart data from NGC:

   :ir_assets:`<isaac_ros_foundationpose> <quickstart.tar.gz>`

#. Download the pre-trained :ir_ngc:`FoundationPose <teams/isaac/models/foundationpose>` models from NGC to the required directory:

   .. code:: bash

      mkdir -p ${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose && \
         cd ${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose && \
         wget 'https://api.ngc.nvidia.com/v2/models/nvidia/isaac/foundationpose/versions/1.0.1_onnx/files/refine_model.onnx' -O refine_model.onnx && \
         wget 'https://api.ngc.nvidia.com/v2/models/nvidia/isaac/foundationpose/versions/1.0.1_onnx/files/score_model.onnx' -O score_model.onnx

Build |package_name|
~~~~~~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: Binary Package

      #. Activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Install the prebuilt Debian package:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-foundationpose

   .. tab:: Build from Source

      #. Install Git LFS:

         .. code:: bash

            sudo apt-get install -y git-lfs && git lfs install

      #. Clone this repository under ``${ISAAC_ROS_WS}/src``:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/src && \
               git clone :ir_clone:`<isaac_ros_pose_estimation>`

      #. Activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Use ``rosdep`` to install the package's dependencies:

         :ir_apt:

         .. code:: bash

            rosdep update && rosdep install --from-paths ${ISAAC_ROS_WS}/src/isaac_ros_pose_estimation/isaac_ros_foundationpose --ignore-src -y

      #. Build the package from source:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/ && \
               colcon build --symlink-install --packages-up-to isaac_ros_foundationpose --base-paths ${ISAAC_ROS_WS}/src/isaac_ros_pose_estimation/isaac_ros_foundationpose

      #. Source the ROS workspace:

         .. note::

            Make sure to repeat this step in **every** terminal created inside the Isaac ROS environment.

            Because this package was built from source, the enclosing workspace must be sourced for ROS to be able to find the package's contents.

         .. code:: bash

            source install/setup.bash

.. _isaac-ros-foundationpose-launch:

Run Launch File
~~~~~~~~~~~~~~~

#. Inside the container, convert models from ``.onnx`` to TensorRT engine plans:

   Convert the refine model:

   .. code:: bash

      /usr/src/tensorrt/bin/trtexec --onnx=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/refine_model.onnx --saveEngine=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/refine_trt_engine.plan --minShapes=input1:1x160x160x6,input2:1x160x160x6 --optShapes=input1:1x160x160x6,input2:1x160x160x6 --maxShapes=input1:42x160x160x6,input2:42x160x160x6

   Convert the score model:

   .. code:: bash

      /usr/src/tensorrt/bin/trtexec --onnx=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/score_model.onnx --saveEngine=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/score_trt_engine.plan --minShapes=input1:1x160x160x6,input2:1x160x160x6 --optShapes=input1:1x160x160x6,input2:1x160x160x6 --maxShapes=input1:252x160x160x6,input2:252x160x160x6

   .. note::
      FoundationPose TensorRT engines are running with FP32 precision in TensorRT 10.3+ versions due to FP16 precision loss.

   .. note::
      The model conversion time varies across different platforms. On Jetson AGX Thor, the engine conversion process takes ~10-15 minutes to complete.

.. tabs::
   .. group-tab:: Rosbag

      #. Complete the :ref:`Isaac ROS RT-DETR tutorial <repositories_and_packages/isaac_ros_object_detection/isaac_ros_rtdetr/index:quickstart>`.

      #. Continuing inside the Isaac ROS environment, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-examples

      #. Run the following launch files to spin up a demo of this package:

         Launch ``isaac_ros_foundationpose``:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=foundationpose interface_specs_file:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_foundationpose/quickstart_interface_specs.json mesh_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_foundationpose/Mustard/textured_simple.obj score_engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/score_trt_engine.plan refine_engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/refine_trt_engine.plan rt_detr_engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/synthetica_detr/sdetr_grasp.plan

      #. Open a **second** terminal and activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Play the rosbag:

         .. code:: bash

            ros2 bag play -l  ${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_foundationpose/quickstart.bag/

   .. group-tab:: RealSense Camera

      #. Ensure that you have already set up your RealSense camera using the :doc:`RealSense setup tutorial </getting_started/sensors/realsense_setup>`. If you have not, please set up the sensor and then restart this quickstart from the beginning.

      #. Complete the :ref:`Isaac ROS RT-DETR tutorial <repositories_and_packages/isaac_ros_object_detection/isaac_ros_rtdetr/index:quickstart>`.

      #. Open a new terminal and activate the Isaac ROS environment:

         .. code:: bash

            isaac-ros activate

      #. Install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-:ir_ros_distro:-isaac-ros-examples ros-:ir_ros_distro:-isaac-ros-realsense

      #. Place the object in front of the camera and run the launch file:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=realsense_mono_rect_depth,foundationpose mesh_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_foundationpose/Mac_and_cheese_0_1/Mac_and_cheese_0_1.obj score_engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/score_trt_engine.plan refine_engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/foundationpose/refine_trt_engine.plan rt_detr_engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/synthetica_detr/sdetr_grasp.plan


Visualize Results
-----------------

#. Open a **new** terminal and activate the Isaac ROS environment:

   .. code:: bash

      isaac-ros activate

   .. tabs::

      .. group-tab:: Rosbag

         #. launch ``RViz2`` to visualize the output

            .. code:: bash

               rviz2 -d  $(ros2 pkg prefix isaac_ros_foundationpose --share)/rviz/foundationpose.rviz

         #.  You should see a ``RViz2`` window open as shown below showing the 3D bounding box overlaid over the input image

            .. figure:: :ir_lfs:`<resources/isaac_ros_docs/repositories_and_packages/isaac_ros_pose_estimation/isaac_ros_foundationpose/foundation_pose_rviz2.png>`
               :width: 600px
               :align: center

      .. group-tab:: RealSense Camera

         #. launch ``RViz2`` to visualize the output

            .. code:: bash

               rviz2 -d  $(ros2 pkg prefix isaac_ros_foundationpose --share)/rviz/foundationpose_realsense.rviz

         #.  You should see a visualization of the 3D pose estimate as shown below:

            .. figure:: :ir_lfs:`<resources/isaac_ros_docs/concepts/pose_estimation/foundationpose/foundationpose_realsense.png>`
               :alt: Isaac ROS FoundationPose RealSense Pose Estimation
               :align: center

.. note::

   FoundationPose is designed to perform pose estimation on previously unseen objects without model retraining.
   As a result, inference requires significant GPU compute for first detection. Tracking once an initial pose estimation is determined can be significantly faster, however.
   Refer to `performance <https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_pose_estimation?tab=readme-ov-file#performance>`__ of the model for your target platform to determine which model to use.

   More powerful discrete GPUs can outperform all other platforms for this task and should be preferred if higher performance is required.
   Interleaving pose estimation with other tasks rather than running continuously can be a more effective solution as well. Finally, if runtime performance
   is critical and offline training resources are available, developers can train :doc:`CenterPose </repositories_and_packages/isaac_ros_pose_estimation/isaac_ros_centerpose/index>`
   for their own target objects using synthetic data generation and/or real-world data for faster pose estimation.


Try More Examples
-----------------

To continue your exploration, check out the following suggested examples:

.. toctree::
   :maxdepth: 1

   Tutorial with Isaac Sim </concepts/pose_estimation/foundationpose/tutorial_isaac_sim.rst>

   Tutorial to launch FoundationPose Tracking </concepts/pose_estimation/foundationpose/tutorial_tracking.rst>

   Tutorial to create your own 3D object mesh in .obj </concepts/pose_estimation/foundationpose/tutorial_create_your_own_mesh.rst>

.. note::

   ``FoundationPose`` expects the origin frame to be at the center of the mesh.

.. toctree::
   :maxdepth: 1

   Tutorial to shift the Mesh origin frame to the center of the Mesh using Meshlab </concepts/pose_estimation/foundationpose/tutorial_shift_mesh_center.rst>

   Tutorial to convert a 3D mesh from .usd to .obj using Isaac Sim </concepts/pose_estimation/foundationpose/tutorial_usd_to_obj.rst>

   Tutorial to simplify a 3D mesh using Meshlab </concepts/pose_estimation/foundationpose/tutorial_simplify_mesh.rst>

   Tutorial to switch the mesh during runtime </concepts/pose_estimation/foundationpose/tutorial_switch_mesh_runtime.rst>


Troubleshooting
---------------

.. include:: /repositories_and_packages/isaac_ros_pose_estimation/_snippets/troubleshooting.rst

Isaac ROS Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with Isaac ROS, please check :doc:`here </troubleshooting/index>`.

Deep Learning Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with using DNN models, please check :doc:`here </troubleshooting/deep_learning>`.

API
----

Usage
~~~~~

.. code:: bash

   ros2 launch isaac_ros_foundationpose isaac_ros_foundationpose.launch.py refine_engine_file_path:=<path to refine model .plan> score_engine_file_path:=<path to score model .plan> mesh_file_path:=<path to object mesh file> launch_rviz:=<enable rviz> launch_bbox_to_mask:=<enable bbox to mask converter> mask_height:=<converted mask height> mask_width:=<converted mask width>

FoundationPose Node
~~~~~~~~~~~~~~~~~~~

ROS Parameters
^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Parameter
     - Type
     - Default
     - Description
   * - ``mesh_file_path``
     - ``string``
     - ``textured_simple.obj``
     - The absolute path to the target object mesh file.
   * - ``min_depth``
     - ``float``
     - ``0.1``
     - Minimum allowed Z-axis value of pointcloud.
   * - ``max_depth``
     - ``float``
     - ``0.4``
     - Minimum allowed X,Y,Z-axis value of pointcloud to threshold.
   * - ``refine_iterations``
     - ``int``
     - ``1``
     - The number of iterations applied to the refinement model can enhance accuracy. However, more iterations will take longer time for processing.
   * - ``symmetry_axes``
     - ``string``
     - ``['']``
     - Rotational symmetries around axes with angles in format 'axis_angle' (e.g., 'x_30' for 30 degrees around x-axis). Valid axes are [x, y, z]. Angles must be explicitly specified (e.g., 'x_180') or use 'full' for full rotation (evaluated at 30Â° increments).
   * - ``symmetry_planes``
     - ``string``
     - ``['']``
     - Deprecated: Use symmetry_axes instead. Adds 180-degree rotational symmetry around specified axes. Format: 'axis' (e.g., 'x' for 180-degree rotation around x-axis). Valid axes are [x, y, z].
   * - ``fixed_axis_angles``
     - ``string``
     - ``['']``
     - Fixed poses constrain the orientation angles during pose estimation by specifying axis-angle pairs (e.g., `x_30`, `y_45`) that force rotations around specific axes to remain at the given values in degrees.
   * - ``fixed_translations``
     - ``string``
     - ``['']``
     - Fixed translation components in format 'axis_value' (e.g., 'x_0.1', 'y_0.2', 'z_0.3'). Valid axes are [x, y, z] for translation in x, y, z directions respectively. If provided, these will override the initial estimated translations.
   * - ``refine_model_file_path``
     - ``string``
     - ``/tmp/refine_model.onnx``
     - The absolute path to the refinement model file in the local file system.
   * - ``refine_engine_file_path``
     - ``string``
     - ``/tmp/refine_trt_engine.plan``
     - The absolute path to either where you want your refinement TensorRT engine plan to be generated or where your pre-generated engine plan file is located.
   * - ``score_model_file_path``
     - ``string``
     - ``/tmp/score_model.onnx``
     - The absolute path to your score model file in the local file system.
   * - ``score_engine_file_path``
     - ``string``
     - ``/tmp/score_trt_engine.plan``
     - The absolute path to either where you want your score TensorRT engine plan to be generated or where your pre-generated engine plan file is located.
   * - ``refine_input_tensor_names``
     - ``string``
     - ``['']``
     - A list of tensor names of refinement model to be bound to specified input bindings names. Bindings occur in sequential order.
   * - ``refine_input_binding_names``
     - ``string``
     - ``['']``
     - A list of input tensor binding names specified by the refinement model.
   * - ``score_input_tensor_names``
     - ``string``
     - ``['']``
     - A list of tensor names of score model to be bound to specified input bindings names. Bindings occur in sequential order.
   * - ``score_input_binding_names``
     - ``string``
     - ``['']``
     - A list of input tensor binding names specified by the score model.
   * - ``refine_output_tensor_names``
     - ``string``
     - ``['']``
     - A list of tensor names to be bound to specified output binding names for the refine model.
   * - ``refine_output_binding_names``
     - ``string``
     - ``['']``
     - A list of output tensor binding names specified by the refine model.
   * - ``score_output_tensor_names``
     - ``string``
     - ``['']``
     - A list of tensor names to be bound to specified output binding names for the score model.
   * - ``score_output_binding_names``
     - ``string``
     - ``['']``
     - A list of output tensor binding names specified by the score model.
   * - ``tf_frame_name``
     - ``string``
     - ``fp_object``
     - Name of the frame that is used when publishing the pose to the TF tree.
   * - ``sync_threshold``
     - ``int``
     - ``0``
     - Synchronization threshold in nanoseconds.
   * - ``debug``
     - ``bool``
     - ``false``
     - When enabled, saves intermediate results to disk for debugging.
   * - ``debug_dir``
     - ``string``
     - ``/tmp/foundationpose``
     - Directory to save intermediate results for debugging.

ROS Topics Subscribed
^^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``pose_estimation/depth_image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input depth image.
   * - ``pose_estimation/segmentation``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input segmentation mask.
   * - ``pose_estimation/image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input color image (rectified).
   * - ``pose_estimation/camera_info``
     - `sensor_msgs/CameraInfo <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/CameraInfo.msg>`__
     - The input image camera_info.

ROS Topics Published
^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``pose_estimation/output``
     - `vision_msgs/Detection3DArray <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection3DArray.msg>`__
     - The output pose estimate.
   * - ``pose_estimation/pose_matrix_output``
     - :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`
     - The output pose matrix, used as the input for next frame tracking.

FoundationPose Tracking Node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ROS Parameters
^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Parameter
     - Type
     - Default
     - Description
   * - ``mesh_file_path``
     - ``string``
     - ``textured_simple.obj``
     - The absolute path to the target object mesh file.
   * - ``min_depth``
     - ``float``
     - ``0.1``
     - Minimum allowed Z-axis value of pointcloud.
   * - ``max_depth``
     - ``float``
     - ``0.4``
     - Minimum allowed X,Y,Z-axis value of pointcloud to threshold.
   * - ``refine_model_file_path``
     - ``string``
     - ``/tmp/refine_model.onnx``
     - The absolute path to the refinement model file in the local file system.
   * - ``refine_engine_file_path``
     - ``string``
     - ``/tmp/refine_trt_engine.plan``
     - The absolute path to either where you want your refinement TensorRT engine plan to be generated or where your pre-generated engine plan file is located.
   * - ``refine_input_tensor_names``
     - ``string``
     - ``['']``
     - A list of tensor names of refinement model to be bound to specified input bindings names. Bindings occur in sequential order.
   * - ``refine_input_binding_names``
     - ``string``
     - ``['']``
     - A list of input tensor binding names specified by the refinement model.
   * - ``refine_output_tensor_names``
     - ``string``
     - ``['']``
     - A list of tensor names to be bound to specified output binding names for the refine model.
   * - ``refine_output_binding_names``
     - ``string``
     - ``['']``
     - A list of output tensor binding names specified by the refine model.
   * - ``tf_frame_name``
     - ``string``
     - ``fp_object``
     - Name of the frame that is used when publishing the pose to the TF tree.

ROS Topics Subscribed
^^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``tracking/depth_image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input depth image.
   * - ``tracking/pose_input``
     - :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`
     - The input pose estimation matrix from last frame.
   * - ``tracking/image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input color image (rectified).
   * - ``tracking/camera_info``
     - `sensor_msgs/CameraInfo <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/CameraInfo.msg>`__
     - The input image camera_info.

ROS Topics Published
^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``tracking/output``
     - `vision_msgs/Detection3DArray <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection3DArray.msg>`__
     - The output pose estimate.
   * - ``tracking/pose_matrix_output``
     - :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`
     - The output pose matrix, used as the input for next frame tracking.

FoundationPose Selector Node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ROS Parameters
^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Parameter
     - Type
     - Default
     - Description
   * - ``reset_period``
     - ``int``
     - ``20000``
     - The reset period in milliseconds.

ROS Topics Subscribed
^^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``depth_image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input depth image.
   * - ``segmentation``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input segmentation mask.
   * - ``image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The input color image (rectified).
   * - ``camera_info``
     - `sensor_msgs/CameraInfo <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/CameraInfo.msg>`__
     - The input image camera_info.
   * - ``tracking/pose_matrix_output``
     - :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`
     - The input pose matrix comes from last frame tracking.
   * - ``pose_estimation/pose_matrix_output``
     - :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`
     - The input pose matrix comes from pose estimation.

ROS Topics Published
^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``pose_estimation/depth_image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The output depth image for pose estimation.
   * - ``pose_estimation/segmentation``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The output segmentation mask for pose estimation.
   * - ``pose_estimation/image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The output color image (rectified) for pose estimation.
   * - ``pose_estimation/camera_info``
     - `sensor_msgs/CameraInfo <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/CameraInfo.msg>`__
     - The output image camera_info for pose estimation.
   * - ``tracking/depth_image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The output segmentation mask for tracking.
   * - ``tracking/image``
     - `sensor_msgs/Image <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/Image.msg>`__
     - The output color image (rectified) for tracking.
   * - ``tracking/camera_info``
     - `sensor_msgs/CameraInfo <https://github.com/ros2/common_interfaces/blob/:ir_ros_distro:/sensor_msgs/msg/CameraInfo.msg>`__
     - The output image camera_info for tracking.
   * - ``tracking/pose_input``
     - :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`
     - The output pose matrix, used as the input for next frame tracking.

FoundationPose Detection2DToMask Node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ROS Parameters
^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Parameter
     - Type
     - Default
     - Description
   * - ``mask_width``
     - ``int``
     - ``640``
     - The output mask width. FoundationPose expects the mask to have the same dimensions as the RGB image.
   * - ``mask_height``
     - ``int``
     - ``480``
     - The output mask height. FoundationPose expects the mask to have the same dimensions as the RGB image.

ROS Topics Subscribed
^^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``detection2_d``
     - `vision_msgs/Detection2D.msg <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection2D.msg>`__
     - The input Detection2D message.
   * - ``detection2_d_array``
     - `vision_msgs/Detection2DArray.msg <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection2DArray.msg>`__
     - The input Detection2D Array message. The mask is created using the highest-scoring detection from the input Detection2D Array message.

ROS Topics Published
^^^^^^^^^^^^^^^^^^^^

.. list-table::
   :header-rows: 1

   * - ROS Topic
     - Interface
     - Description
   * - ``segmentation``
     - `sensor_msgs/Image <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection3DArray.msg>`__
     - The output binary segmentation mask.

.. |package_name| replace:: ``isaac_ros_foundationpose``