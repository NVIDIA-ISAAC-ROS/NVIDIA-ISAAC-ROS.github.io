==============
|package_name|
==============

:ir_github:`<isaac_ros_object_detection> <isaac_ros_yolov8>`

Quickstart
----------

Set Up Development Environment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /_snippets/set_up_dev_env.rst

Download Quickstart Assets
~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Download quickstart data from NGC:

   :ir_assets:`<isaac_ros_yolov8> <quickstart.tar.gz>`

2. Download the model of your choice from `Ultralytics YOLOv8 <https://github.com/ultralytics/ultralytics#models>`__.
   For this example, we use `YOLOv8s <https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt>`__.

   .. code:: bash

      cd Downloads && \
         wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt

3. Convert the PyTorch model (``.pt``) to a general ONNX model (``.onnx``). Export to `ONNX <https://onnx.ai/>`__ following instructions given below or `here <https://docs.ultralytics.com/modes/export/>`__.
   Arguments can be specified for FP16 quantization during this step. This ONNX model is converted to a TensorRT engine 
   file and used with the Isaac ROS TensorRT node for inference. You can use `netron <https://netron.app/>`__ to 
   visualize the ONNX model and note input and output names and dimensions.

   This can be done by first installing ``ultralytics`` and ``onnx`` via pip:

   .. code:: bash

      pip3 install ultralytics
      pip3 install onnx
   
   Afterwards, convert the model from a ``.pt`` file to a ``.onnx`` model using ``ultralytics``. This can be done by running:
   
   .. code:: bash
   
      python3

   Then within ``python3``, export the model:

   .. code:: python

      >> from ultralytics import YOLO
      >> model = YOLO('yolov8s.pt')
      >> model.export(format='onnx')

   Exit the interactive python shell and copy the generated ``.onnx`` model into the designated location for Isaac ROS (``${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8``):

   .. code:: bash

      mkdir -p ${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8
      cp yolov8s.onnx ${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8 

Build |package_name|
~~~~~~~~~~~~~~~~~~~~

.. tabs::

   .. tab:: Binary Package

      1. Launch the Docker container using the ``run_dev.sh`` script:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/src/isaac_ros_common && \
            ./scripts/run_dev.sh

      2. Install the prebuilt Debian package:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-humble-isaac-ros-yolov8 ros-humble-isaac-ros-dnn-image-encoder ros-humble-isaac-ros-tensor-rt  

   .. tab:: Build from Source

      1. Clone this repository under ``${ISAAC_ROS_WS}/src``:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/src && \
               git clone :ir_clone:`<isaac_ros_object_detection>`
     
      2. Launch the Docker container using the ``run_dev.sh`` script:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/src/isaac_ros_common && \
            ./scripts/run_dev.sh

      3. Use ``rosdep`` to install the package's dependencies:

         :ir_apt:

         .. code:: bash

            rosdep update && rosdep install --from-paths ${ISAAC_ROS_WS}/src/isaac_ros_object_detection/isaac_ros_yolov8 --ignore-src -y

      4. Build the package from source:

         .. code:: bash

            cd ${ISAAC_ROS_WS} && \
               colcon build --symlink-install --packages-up-to isaac_ros_yolov8 --base-paths ${ISAAC_ROS_WS}/src/isaac_ros_object_detection/isaac_ros_yolov8

      5. Source the ROS workspace:

         .. note::

            Make sure to repeat this step in **every** terminal created inside the Docker container.
            
            Since this package was built from source, the enclosing workspace must be sourced for ROS to be able to find the package's contents.

         .. code:: bash

            source install/setup.bash

Run Launch File
~~~~~~~~~~~~~~~

1. Enter the Docker container in Jetson:

   .. code:: bash

      cd ${ISAAC_ROS_WS}/src/isaac_ros_common && \
         ./scripts/run_dev.sh

.. tabs::

   .. tab:: Rosbag

      1. Continuing inside the Docker container, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-humble-isaac-ros-examples

      2. Run the following launch file to spin up a demo of this package using the quickstart rosbag:

         .. code:: bash

            cd /workspaces/isaac_ros-dev && \
            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=yolov8 interface_specs_file:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_yolov8/quickstart_interface_specs.json \
               model_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.onnx engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan

      3. Open a **second** terminal inside the Docker container:

         .. code:: bash

            cd ${ISAAC_ROS_WS}/src/isaac_ros_common && \
            ./scripts/run_dev.sh

      4. Run the rosbag file to simulate an image stream:

         .. code:: bash

            ros2 bag play -l ${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_yolov8/quickstart.bag

   .. tab:: RealSense Camera

      #. Ensure that you have already set up your RealSense camera using the :doc:`RealSense setup tutorial </getting_started/hardware_setup/sensors/realsense_setup>`. If you have not, please set up the sensor and then restart this quickstart from the beginning.

      2. Continuing inside the Docker container, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-humble-isaac-ros-examples ros-humble-isaac-ros-realsense
      
      3. Run the following launch file to spin up a demo of this package using a RealSense camera:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=realsense_mono_rect,yolov8 \
               model_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.onnx engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan

   .. tab:: Hawk Camera
         
      #. Ensure that you have already set up your Hawk camera using the :doc:`Hawk setup tutorial </getting_started/hardware_setup/sensors/hawk_setup>`. If you have not, please set up the sensor and then restart this quickstart from the beginning.

      2. Continuing inside the Docker container, install the following dependencies:

         :ir_apt:

         .. code:: bash

            sudo apt-get install -y ros-humble-isaac-ros-examples ros-humble-isaac-ros-argus-camera
      
      3. Run the following launch file to spin up a demo of this package using a Hawk camera:

         .. code:: bash

            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py launch_fragments:=argus_mono,rectify_mono,yolov8 \
               model_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.onnx engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan

   .. tab:: ZED Camera

      #. Ensure that you have already set up your ZED camera using :doc:`ZED setup tutorial </getting_started/hardware_setup/sensors/zed_setup>`.

      #. Continuing inside the Docker container, install dependencies:

         :ir_apt:

         .. code:: bash
            
            sudo apt-get install -y ros-humble-isaac-ros-examples ros-humble-isaac-ros-stereo-image-proc ros-humble-isaac-ros-zed
      
      #. Run the following launch file to spin up a demo of this package using a ZED Camera:

         .. code:: bash
            
            ros2 launch isaac_ros_examples isaac_ros_examples.launch.py \
            launch_fragments:=zed_mono_rect,yolov8 \
            model_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.onnx engine_file_path:=${ISAAC_ROS_WS}/isaac_ros_assets/models/yolov8/yolov8s.plan \
            interface_specs_file:=${ISAAC_ROS_WS}/isaac_ros_assets/isaac_ros_yolov8/zed2_quickstart_interface_specs.json
         
         .. note::
            
            If you are using the `ZED X` series, replace  `zed2_quickstart_interface_specs.json` with `zedx_quickstart_interface_specs.json` in the above command.


Visualize Results
~~~~~~~~~~~~~~~~~

1. Open a **new** terminal inside the Docker container:

   .. code:: bash

      cd ${ISAAC_ROS_WS}/src/isaac_ros_common && \
         ./scripts/run_dev.sh

2. Run the YOLOv8 visualization script:

   .. code:: bash

      ros2 run isaac_ros_yolov8 isaac_ros_yolov8_visualizer.py

3. Open **another** terminal inside the Docker container:

   .. code:: bash

      cd ${ISAAC_ROS_WS}/src/isaac_ros_common && \
         ./scripts/run_dev.sh
         
4. Visualize and validate the output of the package with ``rqt_image_view``:
   
   .. code:: bash

      ros2 run rqt_image_view rqt_image_view /yolov8_processed_image
      
   Your output should look like this:

   .. figure:: :ir_lfs:`<resources/isaac_ros_docs/repositories_and_packages/isaac_ros_object_detection/isaac_ros_yolov8/people-cycling-results.png>`
      :alt: RQT showing detection of people cycling and bikes
      :align: center

Troubleshooting
---------------

Isaac ROS Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with Isaac ROS, please check :doc:`here </troubleshooting/index>`.

Deep Learning Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For solutions to problems with using DNN models, please check :doc:`here </troubleshooting/deep_learning>`.

API
----

Usage
-----

.. code:: bash

   ros2 launch isaac_ros_yolov8 isaac_ros_yolov8_visualize.launch.py model_file_path:=<model_file_path> engine_file_path:=<engine_file_path> input_binding_names:=<input_binding_names> output_binding_names:=<output_binding_names> network_image_width:=<network_image_width> network_image_height:=<network_image_height> force_engine_update:=<force_engine_update> image_mean:=<image_mean> image_stddev:=<image_stddev> confidence_threshold:=<confidence_threshold> nms_threshold:=<nms_threshold>

Yolov8DecoderNode
~~~~~~~~~~~~~~~~~

ROS Parameters
^^^^^^^^^^^^^^

=============================== ============ ============================= ===========================================================================================================================================================================================================================
ROS Parameter                   Type         Default                       Description
=============================== ============ ============================= ===========================================================================================================================================================================================================================
``tensor_name``                 ``string``   ``"output_tensor"``           Name of the inferred output tensor published by the Managed NITROS Publisher. The decoder uses this name to get the output tensor.
``confidence_threshold``        ``float``    ``0.25``                      Detection confidence threshold. Used to filter candidate detections during Non-Maximum Suppression (NMS).
``nms_threshold``               ``float``    ``0.45``                      NMS IOU threshold.
=============================== ============ ============================= ===========================================================================================================================================================================================================================


ROS Topics Subscribed
^^^^^^^^^^^^^^^^^^^^^

============== ==================================================================================================================================================================== ===================================================================================================
ROS Topic      Interface                                                                                                                                                            Description
============== ==================================================================================================================================================================== ===================================================================================================
``tensor_sub`` :ir_repo:`isaac_ros_tensor_list_interfaces/TensorList <isaac_ros_common> <isaac_ros_tensor_list_interfaces/msg/TensorList.msg>`                                      Tensor list from the managed NITROS subscriber that represents the inferred aligned bounding boxes.
============== ==================================================================================================================================================================== ===================================================================================================

ROS Topics Published
^^^^^^^^^^^^^^^^^^^^

======================== =============================================================================================================================== ==================================================
ROS Topic                Interface                                                                                                                       Description
======================== =============================================================================================================================== ==================================================
``detections_output``    `vision_msgs/Detection2DArray <https://github.com/ros-perception/vision_msgs/blob/ros2/vision_msgs/msg/Detection2DArray.msg>`__ Aligned image bounding boxes with detection class.
======================== =============================================================================================================================== ==================================================

.. |package_name| replace:: ``isaac_ros_yolov8``
