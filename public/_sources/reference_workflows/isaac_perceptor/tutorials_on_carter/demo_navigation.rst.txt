Tutorial: Autonomous Navigation with Isaac Perceptor and Nav2
=============================================================

This tutorial will enable you to run autonomous navigation with the Nova Carter robot.
The tutorial uses the Isaac Perceptor stack for local camera-based 3D perception,
AMCL for lidar localization and Nav2 for navigation.

For this tutorial, it is assumed that you have successfully completed the
:doc:`perceptor tutorial</reference_workflows/isaac_perceptor/tutorials_on_carter/demo_perceptor>`.

Instructions
------------

1. SSH into the robot (:ref:`instructions<nova-carter-ssh-setup>`).

2. Make sure you have successfully connected the PS5 joystick to the robot
   (:ref:`instructions<nova-carter-joystick-setup>`).

3. Build/install the required packages:

.. run_ros_app::
    :repo_setup: <nova-carter-dev-setup>
    :repo: nova_carter
    :image: isaac/nova_carter_bringup:release_3.0-aarch64
    :package: nova_carter_bringup
    :app: navigation.launch.py map_yaml_path:=<path_to_map_yaml>
    :asset_packages: isaac_ros_ess_models_install:isaac_ros_peoplesemseg_models_install:isaac_ros_peoplesemseg_models_install
    :asset_scripts: install_ess_models.sh:install_peoplesemsegnet_vanilla.sh:install_peoplesemsegnet_shuffleseg.sh

.. note::
   Make sure to replace ``<path_to_map_yaml>`` with the path of the map
   YAML file. If you do not have a map of your environment you can create
   one using this :doc:`tutorial</robots/nova_carter/demo_lidar_mapping>`

.. note::
   Some users have reported sporadic WiFi connection issues and the robot
   sometimes not responding to goal poses when using the pre-built docker
   images. We are actively working on improving these issues.


Visualizing the Outputs and Sending Goals
-----------------------------------------

1. Make sure you complete :ref:`visualization-setup-carter`.

2. Open Foxglove Studio on your remote machine.
   Open the ``nova_carter_navigation.json`` layout file downloaded in the previous step.

3. Validate that you can see a visualization of the map, local costmap, and
   footprint of the robot. You should expect a visualization similar to the
   following:

.. TODO(lgulich): Update image.

.. figure::
    :ir_lfs:`<resources/isaac_ros_docs/robots/nova_carter/foxglove_nav2.png>`
    :width: 800px
    :alt: foxglove_nav2
    :align: center

4. You can send a goal pose setpoint to Nav2 using the
   `pose publish button <https://docs.foxglove.dev/docs/visualization/panels/3d/#publish>`_
   in Foxglove as shown below:

.. note::

    It is important to ensure that the Foxglove "Display frame" in the 
    `3D panel <https://docs.foxglove.dev/docs/visualization/panels/3d/>`_ is set
    to "map" before sending the goals. If goals are sent and the robot does not
    move, it is best to check first that the correct "Display frame" has been
    set.

.. figure::
    :ir_lfs:`<resources/isaac_ros_docs/robots/nova_carter/foxglove_nav2_galileo.gif>`
    :width: 800px
    :alt: foxglove_nav2_galileo
    :align: center

.. note::

    You can also use the joystick to override Nav2 autonomous control at any point.


AMCL Tuning
-----------
Compared to the default parameters of AMCL we made the following changes to
improve localization performance:

1. Used the ``beam`` laser_model_type instead of the default "likelihood_field"
   laser_model_type

2. Reduced the ``z_hit`` Gaussian model sigma(``sigma_hit``) to 0.1

3. Set the ``beam`` laser_model_type weights to ``z_hit``: 0.5, ``z_max``: 0.05,
   ``z_rand``: 0.4 and ``z_short``: 0.05

4. Increased the ``max_beams`` parameter from ``60`` to ``360`` to allow for
   more detailed features to be used by AMCL

5. Decreased all odometry process noise from ``0.2`` to ``0.1`` to allow higher
   confidence in the robot's wheel odometry
