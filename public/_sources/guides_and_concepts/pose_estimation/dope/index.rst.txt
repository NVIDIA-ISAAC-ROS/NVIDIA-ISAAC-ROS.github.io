DOPE
=========================

`DOPE <https://github.com/NVlabs/Deep_Object_Pose>`__ (Deep Object Pose Estimation) requires a pre-trained model. Input images may need to be cropped and resized to maintain the aspect ratio and match the input resolution of DOPE. After DOPE has produced an estimate, the DNN decoder will use the specified object type to transform using belief maps to output object poses.

NVLabs has provided a DOPE pre-trained model using the `HOPE <https://github.com/swtyree/hope-dataset>`__ dataset. HOPE stands for household objects for pose estimation and is a research-oriented dataset using toy grocery objects and 3D textured meshes of the objects for training on synthetic data. To use DOPE for other objects that are relevant to your application, it needs to be trained with another dataset targeting these objects. For example, DOPE has been trained to detect dollies for use with a mobile robot that navigates under, lifts, and moves that type of dolly.


Examples
~~~~~~~~~~~~~~~~~

To continue your exploration, check out the following suggested
examples:

-  :doc:`DOPE with Triton </guides_and_concepts/pose_estimation/dope/tutorial_triton>`
-  :doc:`DOPE with non-standard input image sizes </guides_and_concepts/pose_estimation/dope/tutorial_custom_size>`
-  :doc:`Train your own DOPE model </guides_and_concepts/pose_estimation/dope/tutorial_custom_model>`

.. toctree::
   :hidden:
   :glob:

   *
